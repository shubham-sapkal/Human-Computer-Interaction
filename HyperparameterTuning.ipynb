{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/Shubham/Hand-Gesture-Recognition-for-HCI-main/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hyperopt\n",
    "# !pip install protobuf\n",
    "# !pip install keras-preprocessing\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lib.kerashyper.kerashypetune import KerasGridSearch\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from lib.data_loader import DataLoader\n",
    "from lib.image import ImageDataGenerator\n",
    "from lib.resnet_model import Resnet3DBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = r'C:\\Users\\Shubham\\Hand-Gesture-Recognition-for-HCI-main\\dataset'\n",
    "labels_csv_path = os.path.join(root_directory, 'labels_extracted.csv')\n",
    "train_csv_path = os.path.join(root_directory, 'train_extracted.csv')\n",
    "val_csv_path = os.path.join(root_directory, 'validation_extracted.csv')\n",
    "train_path = os.path.join(root_directory, 'Train')\n",
    "val_path = os.path.join(root_directory, 'Validation')\n",
    "\n",
    "data = DataLoader(labels_csv_path, train_csv_path, val_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shubham\\\\Hand-Gesture-Recognition-for-HCI-main\\\\dataset\\\\Train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 64\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)\n",
    "BATCH_SIZE = 32\n",
    "N_FRAMES = 16\n",
    "SKIP = 1\n",
    "INPUT_SHAPE = (N_FRAMES,) + TARGET_SIZE + (3,)\n",
    "N_CLASSES = 10\n",
    "EPOCHS = 1\n",
    "WORKERS = 2\n",
    "\n",
    "param_grid = {\n",
    "    'drop_rate': [0.2, 0.5],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(params):\n",
    "\n",
    "    model = Resnet3DBuilder.build_resnet_101(\n",
    "        input_shape=INPUT_SHAPE, \n",
    "        num_outputs=N_CLASSES,\n",
    "        drop_rate=params['drop_rate']\n",
    "    )\n",
    "\n",
    "    optimizer = SGD(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 video folders belonging to 10 classes.\n",
      "Found 2000 video folders belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "image_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = image_datagen.flow_video_from_dataframe(\n",
    "    dataframe=data.train_df, \n",
    "    directory=train_path, \n",
    "    path_classes=labels_csv_path, \n",
    "    x_col='video_id', \n",
    "    y_col='label', \n",
    "    target_size=TARGET_SIZE, \n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    nb_frames=N_FRAMES, \n",
    "    skip=SKIP\n",
    ")\n",
    "\n",
    "validation_generator = image_datagen.flow_video_from_dataframe(\n",
    "    dataframe=data.val_df, \n",
    "    directory=val_path, \n",
    "    path_classes=labels_csv_path, \n",
    "    x_col='video_id', \n",
    "    y_col='label', \n",
    "    target_size=TARGET_SIZE, \n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    nb_frames=N_FRAMES, \n",
    "    skip=SKIP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 trials detected for ('drop_rate', 'learning_rate')\n",
      "\n",
      "***** (1/4) *****\n",
      "Search({'drop_rate': 0.2, 'learning_rate': 0.01})\n",
      "250/250 [==============================] - 6980s 28s/step - loss: 12.8840 - accuracy: 0.1124 - val_loss: 12.8311 - val_accuracy: 0.1220\n",
      "SCORE: 0.122 at epoch 1\n",
      "\n",
      "***** (2/4) *****\n",
      "Search({'drop_rate': 0.2, 'learning_rate': 0.001})\n",
      "250/250 [==============================] - 7354s 29s/step - loss: 12.8970 - accuracy: 0.1000 - val_loss: 12.8494 - val_accuracy: 0.1195\n",
      "SCORE: 0.1195 at epoch 1\n",
      "\n",
      "***** (3/4) *****\n",
      "Search({'drop_rate': 0.5, 'learning_rate': 0.01})\n",
      "250/250 [==============================] - 5918s 24s/step - loss: 12.9096 - accuracy: 0.1010 - val_loss: 12.8343 - val_accuracy: 0.1055\n",
      "SCORE: 0.1055 at epoch 1\n",
      "\n",
      "***** (4/4) *****\n",
      "Search({'drop_rate': 0.5, 'learning_rate': 0.001})\n",
      "250/250 [==============================] - 9134s 36s/step - loss: 12.9044 - accuracy: 0.0997 - val_loss: 12.8425 - val_accuracy: 0.0985\n",
      "SCORE: 0.0985 at epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lib.kerashyper.kerashypetune.KerasGridSearch at 0x2b98a34a890>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kgs = KerasGridSearch(\n",
    "    hypermodel=define_model, \n",
    "    param_grid=param_grid,\n",
    "    monitor='val_accuracy', \n",
    "    greater_is_better=True, \n",
    "    tuner_verbose=1\n",
    ")\n",
    "\n",
    "n_sample_train = len(data.train_df)\n",
    "n_sample_val = len(data.val_df)\n",
    "\n",
    "kgs.search(\n",
    "    x=train_generator, \n",
    "    steps_per_epoch=n_sample_train/BATCH_SIZE,\n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=n_sample_val/BATCH_SIZE,\n",
    "    epochs=1, \n",
    "    workers=WORKERS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drop_rate': 0.2,\n",
       " 'learning_rate': 0.01,\n",
       " 'epochs': 1,\n",
       " 'steps_per_epoch': 250.0,\n",
       " 'batch_size': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kgs.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "94d428781488fc80efbcf1813aebef3ab29b8f7306bf7c2f915b598b7233f7c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
